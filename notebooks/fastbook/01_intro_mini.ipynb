{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a308f8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# COURSE  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497b424f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fd9ec",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* https://course.fast.ai/\n",
    "* https://course.fast.ai/Lessons/lesson1.html\n",
    "* https://github.com/fastai/fastbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af600b29",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Авторы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055191e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- dev\n",
    "- math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf404b08",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Организация курса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abfb23f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- лекции\n",
    "- книга в виде notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c2333",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Подход к обучению"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da67846e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Подход сверху-вниз (learn with a context in place)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ef0ff",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d5eaca",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Отличие от классического ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89745a0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/dl.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2b895e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Мифы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854b23b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/myths.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed2a61a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## PyTorch vs TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ce1a1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/pytorch.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e00b43",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179cc4f5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Простой и быстрый\n",
    "* Лучший фреймворк для низкоуровневой разработки\n",
    "* fastai - самая популярная высокоуровневая библиотека поверх PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c517a52a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Нейрон"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632e24d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Модель нейрона была описана еще 1943 году\n",
    "<img src=\"images/neuron.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32a943f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 80-ые"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee9ed70",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Нейронки с 2 слоями нейронов\n",
    "* Теория расходилась с практикой - 2 слоев было мало"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5b360",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Теорема Цыбенко, Универсальная теорема аппроксимации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2eda04",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Теорема, доказанная Джорджем Цыбенко в 1989 году, которая утверждает, что искусственная нейронная сеть прямой связи (англ. feed-forward; в которых связи не образуют циклов) с одним скрытым слоем может аппроксимировать любую непрерывную функцию многих переменных с любой точностью."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c3551",
   "metadata": {},
   "source": [
    "# Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c37ad",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Мега-задача"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5f33f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. A dataset called the [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) that contains 7,349 images of cats and dogs from 37 different breeds will be downloaded from the fast.ai datasets collection to the GPU server you are using, and will then be extracted.\n",
    "2. A *pretrained model* that has already been trained on 1.3 million images, using a competition-winning model will be downloaded from the internet.\n",
    "3. The pretrained model will be *fine-tuned* using the latest advances in transfer learning, to create a model that is specially customized for recognizing dogs and cats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9185cfe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93bbeec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b5395c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "URLs.PETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc3570c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakorzhnev/mlc/env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sakorzhnev/mlc/env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.187227</td>\n",
       "      <td>0.025155</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>02:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.048267</td>\n",
       "      <td>0.020680</td>\n",
       "      <td>0.008119</td>\n",
       "      <td>05:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Labels cats based on a filename rule provided by the dataset creators\n",
    "# \n",
    "# Computer vision datasets are normally structured in such a way \n",
    "# that the label for an image is part of the filename, or path—most commonly the parent folder name.\n",
    "def is_cat(x): return x[0].isupper()\n",
    "\n",
    "dls = ImageDataLoaders.from_name_func( # image loader\n",
    "    path, \n",
    "    get_image_files(path), \n",
    "    valid_pct=0.2, # validation set, selected randomly\n",
    "    seed=42,\n",
    "    label_func=is_cat, \n",
    "    # a transform contains code that is applied automatically during training\n",
    "    item_tfms=Resize(224) \n",
    ")\n",
    "\n",
    "learn = vision_learner(\n",
    "    dls, \n",
    "    # convolutional neural network (CNN) \n",
    "    # current state-of-the-art approach to creating computer vision models\n",
    "    # \n",
    "    # Models using architectures with more layers take longer to train, and are more prone to overfitting\n",
    "    # But when using more data, they can be quite a bit more accurate.\n",
    "    resnet34, \n",
    "    # tells you what percentage of images in the validation set are being classified incorrectly. \n",
    "    # Another common metric for classification is accuracy (which is just 1.0 - error_rate).\n",
    "    metrics=error_rate,\n",
    "    pretrained=True # default\n",
    ")\n",
    "\n",
    "learn.fine_tune(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abc0f31b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06337381882d409c9f41f2dbcd490196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uploader = widgets.FileUpload()\n",
    "uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e95d4c59",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this a cat?: True.\n",
      "Probability it's a cat: 1.000000\n"
     ]
    }
   ],
   "source": [
    "img = PILImage.create(uploader.data[0])\n",
    "is_cat,_,probs = learn.predict(img)\n",
    "print(f\"Is this a cat?: {is_cat}.\")\n",
    "print(f\"Probability it's a cat: {probs[1].item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c792bbdd",
   "metadata": {},
   "source": [
    "## Overfitting x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676dd8b",
   "metadata": {},
   "source": [
    "<img src=\"images/att_00000.png\" alt=\"Example of overfitting\" caption=\"Example of overfitting\" id=\"img_overfit\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a84b94",
   "metadata": {},
   "source": [
    "* You will learn many methods to avoid overfitting in this book. \n",
    "* However, you should only use those methods **after you have confirmed that overfitting is actually occurring** (i.e., you have actually observed the validation accuracy getting worse during training). \n",
    "* We often see practitioners using over-fitting avoidance techniques even when they have enough data that they didn't need to do so, ending up with a model that may be less accurate than what they could have achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e4d28",
   "metadata": {},
   "source": [
    "## Разница между error_rate и loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f0727e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 5.0.1 (20220820.1526)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"489pt\" height=\"134pt\"\n",
       " viewBox=\"0.00 0.00 489.18 134.36\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 130.36)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-130.36 485.18,-130.36 485.18,4 -4,4\"/>\n",
       "<!-- model -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"217.09,-79.36 141.09,-79.36 137.09,-75.36 137.09,-29.36 213.09,-29.36 217.09,-33.36 217.09,-79.36\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"213.09,-75.36 137.09,-75.36\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"213.09,-75.36 213.09,-29.36\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"213.09,-75.36 217.09,-79.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"177.09\" y=\"-50.66\" font-family=\"Times,serif\" font-size=\"14.00\">architecture</text>\n",
       "</g>\n",
       "<!-- predictions -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>predictions</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"340.14\" cy=\"-54.36\" rx=\"50.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"340.14\" y=\"-50.66\" font-family=\"Times,serif\" font-size=\"14.00\">predictions</text>\n",
       "</g>\n",
       "<!-- model&#45;&gt;predictions -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>model&#45;&gt;predictions</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M217.49,-54.36C236.29,-54.36 259.19,-54.36 280.02,-54.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"280.06,-57.86 290.06,-54.36 280.06,-50.86 280.06,-57.86\"/>\n",
       "</g>\n",
       "<!-- inputs -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>inputs</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"50.05\" cy=\"-74.36\" rx=\"32.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.05\" y=\"-70.66\" font-family=\"Times,serif\" font-size=\"14.00\">inputs</text>\n",
       "</g>\n",
       "<!-- inputs&#45;&gt;model -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>inputs&#45;&gt;model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M81.64,-69.47C95.15,-67.31 111.38,-64.71 126.54,-62.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"127.51,-65.67 136.83,-60.64 126.4,-58.76 127.51,-65.67\"/>\n",
       "</g>\n",
       "<!-- loss -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>loss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"454.18\" cy=\"-83.36\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"454.18\" y=\"-79.66\" font-family=\"Times,serif\" font-size=\"14.00\">loss</text>\n",
       "</g>\n",
       "<!-- predictions&#45;&gt;loss -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>predictions&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M381.27,-64.75C393.51,-67.91 406.85,-71.37 418.68,-74.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"418.07,-77.89 428.63,-77 419.82,-71.11 418.07,-77.89\"/>\n",
       "</g>\n",
       "<!-- parameters -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>parameters</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"50.05\" cy=\"-20.36\" rx=\"50.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.05\" y=\"-16.66\" font-family=\"Times,serif\" font-size=\"14.00\">parameters</text>\n",
       "</g>\n",
       "<!-- parameters&#45;&gt;model -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>parameters&#45;&gt;model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M90.61,-31.12C102.13,-34.25 114.85,-37.71 126.88,-40.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"126.22,-44.42 136.78,-43.67 128.05,-37.67 126.22,-44.42\"/>\n",
       "</g>\n",
       "<!-- labels -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>labels</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"340.14\" cy=\"-108.36\" rx=\"31.4\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"340.14\" y=\"-104.66\" font-family=\"Times,serif\" font-size=\"14.00\">labels</text>\n",
       "</g>\n",
       "<!-- labels&#45;&gt;loss -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>labels&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M369.41,-102.05C384.3,-98.73 402.69,-94.63 418.44,-91.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"419.35,-94.49 428.35,-88.9 417.83,-87.66 419.35,-94.49\"/>\n",
       "</g>\n",
       "<!-- loss&#45;&gt;parameters -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>loss&#45;&gt;parameters</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M440.88,-67.53C429.39,-54.1 410.95,-35.74 390.18,-27.36 295.38,10.89 173.21,0.49 104.38,-10.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"103.53,-6.77 94.21,-11.81 104.64,-13.68 103.53,-6.77\"/>\n",
       "<text text-anchor=\"middle\" x=\"253.59\" y=\"-6.16\" font-family=\"Times,serif\" font-size=\"14.00\">update</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x159b29c60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "#caption Detailed training loop\n",
    "#id detailed_loop\n",
    "gv('''ordering=in\n",
    "model[shape=box3d width=1 height=0.7 label=architecture]\n",
    "inputs->model->predictions; parameters->model; labels->loss; predictions->loss\n",
    "loss->parameters[constraint=false label=update]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccc953",
   "metadata": {},
   "source": [
    "* The entire purpose of **loss** is to define a **\"measure of performance\"** that the training system can use to **update weights automatically**. \n",
    "* In other words, a **good choice for loss** is a choice that is **easy for stochastic gradient descent to use**. \n",
    "* But a **metric** is defined for human consumption, so **a good metric is** one that is **easy for you to understand**, and that hews as closely as possible to what you want the model to do. \n",
    "* At times, you might decide that the **loss** function is a **suitable metric**, but that is **not necessarily** the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1fb82a",
   "metadata": {},
   "source": [
    "## Pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e60ca",
   "metadata": {},
   "source": [
    "* Модель уже с весами, установленными в результате тренировки на 1.3 million photos на [*ImageNet* dataset](http://www.image-net.org/)\n",
    "* You should nearly always use a pretrained model, так как она уже полезна\n",
    "* For instance, parts of pretrained models will handle edge, gradient, and color detection, which are needed for many tasks.\n",
    "\n",
    "When using a pretrained model, `vision_learner` will **remove the last layer**, since that is always specifically **customized to the original training task** (i.e. ImageNet dataset classification), and replace it with one or more **new layers** with **randomized** weights, of an appropriate size for the dataset you are working with. This last part of the model is known as the *head*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6395041",
   "metadata": {},
   "source": [
    "## Pretrained model. Преимущества"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc414db5",
   "metadata": {},
   "source": [
    "* Allow us to train more accurate models, \n",
    "* more quickly, \n",
    "* with less data, \n",
    "* and less time and money\n",
    "\n",
    "В большинстве обучающих материалов эта тема не затрагивается и не раскрывается. \n",
    "\n",
    "> jargon: Transfer learning: Using a pretrained model for a task different to what it was originally trained for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204a157",
   "metadata": {},
   "source": [
    "## fine_tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac98c7",
   "metadata": {},
   "source": [
    "As we've discussed, the **architecture** only describes a **template** for a mathematical function; it doesn't actually do anything until we provide values for the millions of parameters it contains.\n",
    "\n",
    "But why is the method called `fine_tune`, and not `fit`? fastai actually *does* have a method called `fit`, which does indeed **fit** a model (**i.e. look at images in the training set multiple times**, each time updating the parameters to make the predictions closer and closer to the target labels). But in this case, we've **started with a pretrained model**, and we **don't want to throw away** all those capabilities that it already has. As you'll learn in this book, there are some important tricks to adapt a pretrained model for a new dataset—a process called **fine-tuning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de4ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "398.999px",
    "width": "556.996px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
